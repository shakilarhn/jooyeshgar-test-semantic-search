# jooyeshgar-test-semantic-searchتوضیحات پیاده سازی

وابستگی‌ها و نصب پروژه
برای اجرای این پروژه، از کتابخانه‌های زیر استفاده شده است که در فایل requirements.txt به صورت کامل آورده شده‌اند:

python-dotenv: برای مدیریت و بارگذاری متغیرهای محیطی از فایل .env به منظور حفظ امنیت اطلاعات حساس مانند API_KEY.

sentence-transformers: جهت تبدیل متون به بردارهای تعبیه‌شده با استفاده از مدل paraphrase-multilingual-MiniLM-L12-v2. این مدل به دلیل پشتیبانی از چندین زبان (از جمله فارسی)، سرعت بالا و مصرف منابع کمتر نسبت به مدل‌های سنگین‌تر انتخاب شده است. اگرچه دقت آن در برخی کاربردها کمتر از مدل‌های پیچیده‌تر است، اما برای دیتاست‌های کم‌حجم و جستجوی معنایی عملکرد قابل قبولی دارد.

typesense: کتابخانه‌ای برای برقراری ارتباط با پایگاه داده برداری Typesense و اجرای عملیات جستجو.

selenium و webdriver-manager: جهت اتوماسیون وب برای استخراج داده‌ها از وب‌سایت‌ها با استفاده از مرورگر در حالت headless و مدیریت زمان‌بندی و خطاهای مربوط به Timeout.

beautifulsoup4: جهت پاکسازی و استخراج داده‌های HTML و حذف تگ‌های غیر ضروری از متون.

clean-text: برای استانداردسازی و پاکسازی متون، شامل حذف شکست‌های خط، URLها، ایمیل‌ها و شماره تلفن‌ها.

pandas و numpy: برای پردازش داده‌ها، خواندن و نوشتن فایل‌های CSV و انجام محاسبات عددی.

scikit-learn: جهت محاسبه شباهت کسینوسی بین بردارهای تعبیه‌شده که پایه رتبه‌بندی نتایج جستجو است.

برای نصب تمامی این وابستگی‌ها کافی است دستور زیر در ترمینال اجرا شود:
pip install -r requirements.txt
روند پیاده‌سازی پروژه
ابتدا از سه دسته مختلف، ۱۰ عدد محصول از هر دسته استخراج و لینک‌های مربوط به آن‌ها در یک لیست ذخیره می‌شود. اطلاعات استخراج‌شده شامل عنوان محصول، توضیحات محصول و لینک محصول در یک فایل CSV ذخیره شده و با استفاده از تکنیک‌هایی مانند WebDriverWait، استفاده از try/except و اجرای مرورگر در حالت headless، مشکلاتی مانند Timeout برطرف و از ایجاد خطا جلوگیری شده است.

سپس فایل CSV حاوی داده‌های استخراج‌شده جهت ورودی به مدل‌های تعبیه‌شده پیش‌پردازش می‌شود. در این مرحله، عملیات پاکسازی شامل حذف تگ‌های HTML با استفاده از BeautifulSoup و استانداردسازی متن‌ها با استفاده از کتابخانه clean-text انجام می‌شود. نتیجه‌ی این مرحله در یک فایل CSV جدید ذخیره می‌گردد تا داده‌های ورودی به مدل Embedding، تمیز و یکپارچه باشد.

تولید بردارهای تعبیه‌شده
برای تولید بردارهای تعبیه‌شده از مدل sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 استفاده شده است. این مدل به دلایل زیر انتخاب شده است:
پشتیبانی از چندین زبان، از جمله فارسی
سبک و سریع بوده و نیاز به منابع محاسباتی نسبتاً کمی دارد
دقت قابل قبولی در جستجوی معنایی ارائه می‌دهد
**در شرایطی که نیاز به دقت بسیار بالا باشد، ممکن است مدل‌های پیچیده‌تری مانند BERT یا GPT مورد نیاز باشد

برای بهینه‌سازی سرعت و کارایی، از دکوریتور @lru_cache(maxsize=1) برای کش کردن مدل استفاده شده است تا مدل تنها یک بار در هر اجرای برنامه بارگذاری شود و از تکرار عملیات بارگذاری جلوگیری شود. علاوه بر این، به منظور یکپارچه‌سازی داده‌ها، ستون‌های عنوان و توضیحات هر محصول در یک رشته‌ی واحد (combined_text) ترکیب می‌شوند که به مدل کمک می‌کند تا درک جامع‌تری از محصول داشته باشد.

ارتباط با پایگاه داده برداری Typesense
پس از تولید بردارهای تعبیه‌شده، ارتباط با پایگاه داده برداری Typesense برقرار می‌شود. در این بخش:

کلاینت Typesense با استفاده از API_KEY و تنظیمات سرور (host، port، protocol) از طریق متغیرهای محیطی مقداردهی اولیه می‌شود.

یک کالکشن به نام products تعریف شده که شامل فیلدهایی نظیر id، Title، Description، URL، combined_text و embedding (با ابعاد 384) است.

داده‌های تولیدشده از طریق عملیات Bulk Import (و به صورت upsert) به این کالکشن ارسال می‌شود. استفاده از Bulk Import باعث کاهش تعداد درخواست‌ها و افزایش سرعت ورود داده‌ها به Typesense می‌شود.
جستجوی برداری با استفاده از multi_search و رتبه‌بندی نتایج
برای جستجوی معنایی:

ورودی کاربر از طریق رابط خط فرمان (CLI) دریافت می‌شود. کوئری وارد شده به بردار تعبیه‌شده تبدیل می‌شود.

از متد multi_search برای ارسال یک درخواست چندگانه استفاده می‌شود. این روش باعث می‌شود که به جای ارسال درخواست‌های جداگانه، چندین درخواست در یک فراخوانی انجام شود و از محدودیت تعداد درخواست‌ها و زمان‌بندی جلوگیری شود.

نتایج جستجو بر اساس شباهت کسینوسی رتبه‌بندی می‌شوند. در این روش، شباهت بین بردار کوئری و بردارهای محصولات با استفاده از cosine similarity محاسبه می‌شود. محصولاتی که بردارشان زاویه کمتری با بردار کوئری دارد، به عنوان نتایج دقیق‌تر نمایش داده می‌شوند.

تنظیماتی مانند threshold (آستانه) می‌تواند به گونه‌ای اعمال شود که تنها محصولاتی با شباهت بالاتر از مقدار مشخص (مثلاً 0.7) به کاربر نمایش داده شود.

رابط کاربری خط فرمان (CLI)
سیستم به گونه‌ای طراحی شده است که کاربران به صورت تعاملی از طریق خط فرمان، کوئری‌های جستجو را وارد کنند. ویژگی‌های مهم این بخش عبارتند از:

اعتبارسنجی ورودی: بررسی می‌شود که کاربر ورودی خالی وارد نکند.
امکان خروج از سیستم با تایپ دستور "exit".
نمایش نتایج جستجو به صورت خوانا (فرمت JSON) برای سهولت مشاهده و استفاده.

استفاده از lru_cache باعث کاهش زمان بارگذاری مدل شده و مصرف منابع را بهینه می‌کند.
پارامترهای مهم مانند batch_size (تعیین تعداد نمونه‌ها در هر دسته جهت پردازش)، connection_timeout_seconds (زمان حداکثر انتظار برای اتصال به Typesense) و سایر تنظیمات به دقت انتخاب شده‌اند تا عملکرد سیستم بهینه شود.
محاسبه شباهت کسینوسی با استفاده از scikit-learn انجام می‌شود که باعث رتبه‌بندی دقیق نتایج بر اساس شباهت معنایی می‌گردد.

راهنمای نصب و راه‌اندازی پروژه
برای راه‌اندازی پروژه مراحل زیر را دنبال کنید:

نصب وابستگی‌ها:

pip install -r requirements.txt
راه‌اندازی Typesense (استفاده از Docker):
docker run -d --name typesense-server \
  -p 8108:8108 \
  -v $(pwd)/typesense-data:/data \
  typesense/typesense:0.24.0 \
  --data-dir /data \
  --api-key=xyz123 \
  --enable-cors
از دستور docker ps برای بررسی اجرای سرویس استفاده کنید.

با دستور زیر، سلامت Typesense را بررسی کنید:

curl http://localhost:8108/health
خروجی مورد انتظار:

{"ok":true}
اجرای پروژه:

python test.py


**در شرایط دیتاست‌های بسیار بزرگ، ممکن است مدل انتخاب‌شده نیاز به مدل‌های پیچیده‌تر داشته باشد.
نیاز به تنظیمات دقیق برای batch_size و connection_timeout_seconds جهت بهینه‌سازی عملکرد در محیط‌های متفاوت.



